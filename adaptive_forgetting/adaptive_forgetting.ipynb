{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e510f4cf",
   "metadata": {},
   "source": [
    "# Vergessen: Die unterschätzte Superkraft\n",
    "\n",
    "Vergessen ist kein Mangel, sondern eine Voraussetzung für Handlungsfähigkeit, weil es Raum für Neues schafft und Überlastung verhindert. Hermann Ebbinghaus entdeckte in seinen bahnbrechenden Experimenten (1885) die mathematische Struktur des Vergessens durch systematische Selbstversuche mit sinnlosen Silben. Seine empirischen Daten zeigen, dass Gedächtnisverlust über Zeit folgt – diese Daten werden heute häufig mit exponentiellen oder Power-Law-Modellen beschrieben.\n",
    "\n",
    "**Moderne Vereinfachung** – Die exponentiell abfallende Vergessenskurve:\n",
    "\n",
    "$$R(t) = e^{-t/S}$$\n",
    "\n",
    "wobei $R$ die Retention (Behaltensleistung), $t$ die Zeit und $S$ die Gedächtnisstabilität ist. Diese Exponentialfunktion ist eine vereinfachte Darstellung. In der Praxis passen komplexere Modelle wie das **Power Law** $R(t) = (1 + t)^{-\\beta}$ oft besser zu empirischen Daten, da sie den langsameren Verfall über längere Zeiträume präziser abbilden.\n",
    "\n",
    "**Spacing Effect und Gedächtniskonsolidierung:** Moderne Forschung erweiterte diese Erkenntnisse um den Spacing Effect – die Beobachtung, dass zeitlich verteilte Wiederholungen die Gedächtnisstabilität durch synaptische Konsolidierung deutlich erhöhen. Nach der **ACT-R-Theorie** (Adaptive Control of Thought–Rational) behält das Gehirn bevorzugt Informationen, die basierend auf ihrer Abrufhistorie statistisch wahrscheinlich wieder benötigt werden – eine adaptive Optimierungsstrategie.\n",
    "\n",
    "**Interferenz und Speichereffizienz:** Neue Erinnerungen konkurrieren beim Abruf mit ähnlichen alten (retroaktive Interferenz) – so werden redundante Inhalte seltener abgerufen und praktisch weniger präsent. Dies betrifft vor allem das deklarative Gedächtnis (Fakten und Ereignisse), während prozedurale Gedächtnisinhalte (motorische Fähigkeiten) weniger anfällig sind.\n",
    "\n",
    "**Relevanz für künstliche Systeme:** Für KI-Agenten ist diese Einsicht entscheidend. Ein System, das jedes Gespräch, jedes Datenfragment und jede Zwischennotiz ohne Selektion ablegt, würde ein chaotisches, überfrachtetes System schaffen, das keine Prioritäten kennt und im entscheidenden Moment mit irrelevanten Details überlastet ist.\n",
    "\n",
    "Die technische Implementierung adaptiven Vergessens folgt mehreren biologisch inspirierten Strategien: zeitbasiertes Decay, relevanzbasierte Konsolidierung und interferenzgesteuerte Selektion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8300bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba71d5",
   "metadata": {},
   "source": [
    "## Adaptives Vergessen – Neurobiologische Grundlagen und praktische Anwendung\n",
    "\n",
    "Dieses Notebook vermittelt die neurobiologischen und mathematischen Grundlagen des Vergessens und zeigt deren praktische Anwendung in Versicherungs- und Finanzsystemen.\n",
    "\n",
    "### Problemstellung und Motivation\n",
    "\n",
    "Vergessen wird häufig als Schwäche oder Defizit betrachtet. Aus neurobiologischer und informationstheoretischer Perspektive ist Vergessen jedoch eine **essenzielle kognitive Funktion**, die mehrere kritische Aufgaben erfüllt:\n",
    "\n",
    "**Warum ist Vergessen notwendig?**\n",
    "\n",
    "1. **Kapazitätsmanagement:** Biologische und künstliche Systeme verfügen über begrenzte Speicher- und Verarbeitungskapazität\n",
    "2. **Interferenzreduktion:** Veraltete oder irrelevante Informationen können aktuelle Entscheidungsprozesse stören\n",
    "3. **Generalisierung:** Selektives Vergessen von Details ermöglicht die Extraktion allgemeiner Muster\n",
    "4. **Energieeffizienz:** Die Aufrechterhaltung synaptischer Verbindungen erfordert kontinuierliche metabolische Investition\n",
    "5. **Adaptivität:** Systeme müssen sich an veränderte Umgebungen anpassen können\n",
    "\n",
    "### Praktische Relevanz im Versicherungswesen\n",
    "\n",
    "Im Versicherungswesen stellt sich die Frage: **Wie lange soll ein Schadensfall die Risikobewertung eines Kunden beeinflussen?**\n",
    "\n",
    "**Praktische Szenarien:**\n",
    "\n",
    "- **Kfz-Versicherung:** Ein Kunde hatte vor 5 Jahren einen Unfall, seitdem schadenfrei. Soll dieser alte Schaden noch die Prämie erhöhen?\n",
    "- **Betrugsdetection:** Betrugsmuster ändern sich kontinuierlich. Alte Muster können neue legitime Verhaltensweisen fälschlicherweise als Betrug klassifizieren\n",
    "- **Risikoprofil-Aktualisierung:** Kundenprofile müssen aktuell bleiben, aber zu häufiges Löschen verliert wertvolle historische Informationen\n",
    "\n",
    "Diese Probleme erfordern **adaptive Vergessensstrategien**, die biologisch inspirierte Mechanismen nutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e9b09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Notebook-Styling laden\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = (\n",
    "    NOTEBOOK_DIR.parent\n",
    "    if (NOTEBOOK_DIR.parent / \"notebook_style.py\").exists()\n",
    "    else NOTEBOOK_DIR\n",
    ")\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"notebook_style\", PROJECT_ROOT / \"notebook_style.py\"\n",
    ")\n",
    "if spec is None or spec.loader is None:\n",
    "    raise ImportError\n",
    "\n",
    "nb_style = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"notebook_style\"] = nb_style\n",
    "spec.loader.exec_module(nb_style)\n",
    "\n",
    "PLOT_COLORS = nb_style.setup_plot_style(\n",
    "    aliases={\n",
    "        'retention': 'primary',\n",
    "        'decay': 'secondary',\n",
    "        'adaptive': 'accent',\n",
    "        'threshold': 'quaternary',\n",
    "    },\n",
    "    cycle_keys=(\"primary\", \"secondary\", \"accent\"),\n",
    ")\n",
    "\n",
    "SEED = int(getattr(nb_style, 'SEED', 42))\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc3393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leaky_integrator",
   "metadata": {},
   "source": [
    "### Adaptives Vergessen – Leaky Integrator\n",
    "\n",
    "Das **Leaky-Integrator Modell** beschreibt, wie Erinnerungen mit der Zeit verblassen:\n",
    "\n",
    "$$m_t = (1-\\lambda) \\cdot m_{t-1} + \\lambda \\cdot x_t$$\n",
    "\n",
    "**Intuition:**\n",
    "- $m_t$ = Gedächtnisspur (Schätzung der aktuellen Rate)\n",
    "- $\\lambda$ = Lernrate (wie schnell wir neue Informationen aufnehmen)\n",
    "- $x_t$ = Neue Beobachtung (z.B. Schadenfrequenz heute)\n",
    "- $(1-\\lambda)$ = Vergessensrate (wie schnell alte Informationen verblassen)\n",
    "\n",
    "**Neurobiologisches Mapping:**\n",
    "- $m_t$ <-> Synapsenstärke (Gedächtnisspur)\n",
    "- $\\lambda$ <-> Lern-/Vergessensrate (neuromodulierbar)\n",
    "- $\\tau \\approx 1/\\lambda$ für kleine $\\lambda$; exakt: $\\tau = -1/\\ln(1-\\lambda)$ <-> Zeitkonstante des Synapsendecay\n",
    "\n",
    "### Adaptive Lernrate\n",
    "\n",
    "Statt einer festen Lernrate $\\lambda$ verwenden wir eine **adaptive Lernrate** $\\lambda_t$, die sich an die Überraschung anpasst:\n",
    "\n",
    "$$\\lambda_t = \\text{clip}(\\lambda_{\\min} + k \\cdot \\frac{|e_t|}{\\sigma_t}, \\lambda_{\\min}, \\lambda_{\\max})$$\n",
    "\n",
    "**Komponenten:**\n",
    "- $e_t = x_t - m_{t-1}$ = Vorhersagefehler (Überraschung)\n",
    "- $\\sigma_t = \\sqrt{m_{t-1}}$ = Poisson-Standardabweichung (Erwartete Variabilität)\n",
    "- $k$ = Überraschungs-Sensitivität (wie stark Überraschung die Lernrate beeinflusst)\n",
    "- $\\lambda_{\\min}, \\lambda_{\\max}$ = Grenzen der Lernrate\n",
    "\n",
    "**Neurobiologisches Mapping:**\n",
    "- $\\lambda_t$ <-> Neuromodulation durch Überraschung/Fehler (z.B. Dopamin-Signal)\n",
    "- Höhere Überraschung -> höhere Plastizität -> schnelleres Lernen\n",
    "- Dies ist biologisch plausibel und entspricht dem Kalman-Filter-Prinzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443a22e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375a1eb",
   "metadata": {},
   "source": [
    "### Praktisches Szenario: Kfz-Versicherung\n",
    "\n",
    "**Szenario:** Eine Versicherung möchte die Schadenfrequenz eines Kunden schätzen, um die Prämie zu berechnen.\n",
    "\n",
    "**Problem:** \n",
    "- Mit fester Lernrate $\\lambda$ reagiert das System zu langsam auf wechselnde Schadenmuster (z.B. wenn der Kunde plötzlich mehr Schäden meldet)\n",
    "- Oder es reagiert zu schnell auf Rauschen (einzelne Ausreißer beeinflussen die Schätzung zu stark)\n",
    "\n",
    "**Lösung:** Adaptive Lernrate, die mit der Überraschung (Vorhersagefehler) steigt.\n",
    "\n",
    "**Interpretation:**\n",
    "- Wenn die beobachtete Schadenfrequenz stark von der Schätzung abweicht -> große Überraschung -> höhere Lernrate\n",
    "- Wenn die beobachtete Schadenfrequenz der Schätzung entspricht -> kleine Überraschung -> niedrigere Lernrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992eb1ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8302d",
   "metadata": {},
   "source": [
    "### Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "leaky_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_claims(t=300, regimes=None, seed=42):\n",
    "    \"\"\"Simuliert Schaden-Claim-Daten.\"\"\"\n",
    "    if regimes is None:\n",
    "        regimes = [(0, 100, 2.0), (100, 200, 5.0), (200, 300, 3.0)]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lam_true = np.zeros(t)\n",
    "    for start, end, rate in regimes:\n",
    "        lam_true[start:end] = rate\n",
    "\n",
    "    x = rng.poisson(lam_true)\n",
    "    return x, lam_true\n",
    "\n",
    "\n",
    "def ema_fixed(x, lam=0.05):\n",
    "    \"\"\"Exponential Moving Average mit fester Lernrate.\"\"\"\n",
    "    m = np.empty_like(x, dtype=float)\n",
    "    m[0] = x[0]\n",
    "    for t in range(1, len(x)):\n",
    "        m[t] = (1 - lam) * m[t-1] + lam * x[t]\n",
    "    return m\n",
    "\n",
    "\n",
    "def ema_adaptive(x, lam_min=0.02, lam_max=0.30, k=0.05):\n",
    "    \"\"\"Exponential Moving Average mit adaptiver Lernrate.\"\"\"\n",
    "    m = np.empty_like(x, dtype=float)\n",
    "    lam_t = np.empty_like(x, dtype=float)\n",
    "    m[0] = x[0]\n",
    "    lam_t[0] = lam_min\n",
    "    eps = 1e-6\n",
    "\n",
    "    for t in range(1, len(x)):\n",
    "        exp_noise = np.sqrt(max(m[t-1], eps))\n",
    "        e_norm = abs(x[t] - m[t-1]) / (exp_noise + eps)\n",
    "        lam = np.clip(lam_min + k * e_norm, lam_min, lam_max)\n",
    "        lam_t[t] = lam\n",
    "        m[t] = (1 - lam) * m[t-1] + lam * x[t]\n",
    "\n",
    "    return m, lam_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473fbf9",
   "metadata": {},
   "source": [
    "### Interaktives Beispiel: adaptive Lernrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demo_code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce26c4f7a79c4203a0e4f58bc79ab2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.05, description='λ fix:', max=0.3, min=0.005, step=0.005), FloatSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(lam_fixed=0.05, lam_min=0.01, lam_max=0.3, k=0.05)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daten einmalig simulieren\n",
    "x, lam_true = simulate_claims(t=300)\n",
    "\n",
    "def plot_comparison(x, lam_true, m_fixed, m_adapt, lam_t):\n",
    "    \"\"\"Visualisiert Beobachtungen Schätzungen und adaptive Lernrate.\"\"\"\n",
    "    T = len(x)\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(20, 7),\n",
    "        sharex=True,\n",
    "        gridspec_kw={'height_ratios': [3, 1]}\n",
    "    )\n",
    "\n",
    "    ax1.step(np.arange(T), x, where='mid', color='gray', alpha=0.5,\n",
    "             label='Beobachtete Schäden/Tag', linewidth=1.5)\n",
    "    ax1.plot(lam_true, 'k--', lw=2, label='Wahre Rate (unbekannt)', alpha=0.7)\n",
    "    ax1.plot(m_fixed, color=PLOT_COLORS['retention'], label='EMA fix', lw=2, alpha=0.8)\n",
    "    ax1.plot(m_adapt, color=PLOT_COLORS['adaptive'], label='EMA adaptiv', lw=2, alpha=0.8)\n",
    "\n",
    "    ax1.set_ylabel('Schadenrate/Tag', fontsize=11, fontweight='bold')\n",
    "    ax1.legend(loc='upper left', ncol=2, fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_title('Kfz-Versicherung: Schadenfrequenz-Schätzung', fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax2.plot(lam_t, color=PLOT_COLORS['adaptive'], lw=2, label='λ_t (adaptiv)')\n",
    "    ax2.set_ylabel('Lernrate λ_t', fontsize=11, fontweight='bold')\n",
    "    ax2.set_xlabel('Tag', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylim(0, max(lam_t) * 1.1)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def update_plot(lam_fixed=0.05, lam_min=0.01, lam_max=0.30, k=0.05):\n",
    "    m_fixed = ema_fixed(x, lam=lam_fixed)\n",
    "    m_adapt, lam_t = ema_adaptive(x, lam_min=lam_min, lam_max=lam_max, k=k)\n",
    "    plot_comparison(x, lam_true, m_fixed, m_adapt, lam_t)\n",
    "\n",
    "\n",
    "# Interaktive Slider\n",
    "interact(\n",
    "    update_plot,\n",
    "    lam_fixed=widgets.FloatSlider(0.05, min=0.005, max=0.30, step=0.005,\n",
    "                                   description='λ fix:'),\n",
    "    lam_min=widgets.FloatSlider(0.01, min=0.001, max=0.20, step=0.001,\n",
    "                                 description='λ_min:'),\n",
    "    lam_max=widgets.FloatSlider(0.30, min=0.05, max=0.60, step=0.01,\n",
    "                                 description='λ_max:'),\n",
    "    k=widgets.FloatSlider(0.05, min=0.00, max=0.20, step=0.005,\n",
    "                           description='k (Überraschung):')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13171dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation",
   "metadata": {},
   "source": [
    "### Interpretation der Ergebnisse\n",
    "\n",
    "**Was beobachten wir?**\n",
    "\n",
    "1. **Feste Lernrate (blau):**\n",
    "   - Mit niedriger $\\lambda$ (z.B. 0.01): Langsame Anpassung, glatte Kurve, aber verzögerter Regimewechsel\n",
    "   - Mit hoher $\\lambda$ (z.B. 0.20): Schnelle Anpassung, aber viel Rauschen\n",
    "\n",
    "2. **Adaptive Lernrate (rot):**\n",
    "   - Passt sich automatisch an: Niedrig bei stabilen Phasen, hoch bei Regimewechseln\n",
    "   - Unten siehst du $\\lambda_t$ über Zeit: Spitzen bei großen Überraschungen\n",
    "\n",
    "3. **Praktische Implikation:**\n",
    "   - Adaptive Lernrate kombiniert die Vorteile beider Welten\n",
    "   - Sie ist robust gegen Rauschen UND reagiert schnell auf echte Änderungen\n",
    "\n",
    "**Praxisleitfaden:**\n",
    "- **Konservativ:** Hohe $\\lambda_{\\min}$, niedrige $\\lambda_{\\max}$, niedriges $k$ -> Weniger Überreaktion\n",
    "- **Aggressiv:** Niedrige $\\lambda_{\\min}$, hohe $\\lambda_{\\max}$, hohes $k$ -> Schnellere Anpassung\n",
    "- **Ausgewogen:** Mittlere Werte (Standard) -> Balance zwischen Stabilität und Responsivität"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea92295",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neurobiologie",
   "metadata": {},
   "source": [
    "### Neurobiologische Grundlagen\n",
    "\n",
    "#### Warum funktioniert dieses Modell biologisch?\n",
    "\n",
    "**1. Leaky Integrator <-> Synapsendecay**\n",
    "- Synapsen verlieren ihre Stärke über Zeit, wenn sie nicht aktiviert werden (Decay)\n",
    "- Das $(1-\\lambda)$ Term modelliert diesen natürlichen Verfall\n",
    "\n",
    "**2. Adaptive Lernrate <-> Neuromodulation**\n",
    "- Das Gehirn moduliert die Plastizität (Lernfähigkeit) basierend auf Überraschung/Fehler\n",
    "- Neurotransmitter wie Dopamin signalisieren Vorhersagefehler und erhöhen die Plastizität\n",
    "- Dies entspricht dem biologischen Prinzip: \"Große Überraschung -> Schnelleres Lernen\"\n",
    "\n",
    "**3. Poisson-Normalisierung <-> Biologische Plausibilität**\n",
    "- Für Ereignisraten (wie Schadenfrequenz) ist Poisson-Verteilung realistisch\n",
    "- Die Normalisierung durch $\\sqrt{m_{t-1}}$ entspricht der biologischen Erwartung von Variabilität\n",
    "\n",
    "**4. Kalman-Filter-Logik**\n",
    "- Dieses Modell ist ein vereinfachter Kalman-Filter\n",
    "- Kalman-Filter sind in Neurowissenschaft und ML etabliert als biologisch plausible Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0d531",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5119ed7",
   "metadata": {},
   "source": [
    "#### Zusammenfassung\n",
    "Das Leaky-Integrator-Modell mit adaptiver Lernrate vereint zentrale neurobiologische Prinzipien – synaptischen Zerfall, neuromodulatorische Anpassung und Kalman-Filter-Dynamik – zu einem konsistenten, praxisorientierten Rechenmodell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
