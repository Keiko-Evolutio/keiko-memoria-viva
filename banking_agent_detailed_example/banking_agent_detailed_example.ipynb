{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599c37c3",
   "metadata": {},
   "source": [
    "# Banking-Agent mit neurobiologisch inspiriertem Gedächtnis\n",
    "\n",
    "## Der Paradigmenwechsel: Vom reaktiven Chatbot zum erinnernden Partner\n",
    "\n",
    "Stellen Sie sich vor, Sie gehen in die Online-Filiale Ihrer Bank. Dort begrüßt Sie kein anonymer Chatbot, sondern ein digitaler Finanzexperte, der Sie mit den Worten anspricht:\n",
    "\n",
    "*„Guten Tag, Herr Müller. Ich erinnere mich, dass wir im letzten Gespräch Ihre Sorgen bezüglich der Inflation besprochen haben. Sie sagten, dass die Ausbildung Ihrer Tochter Lisa in fünf Jahren finanziert werden muss und Sie deshalb eher vorsichtig investieren möchten. Sollen wir unser Gespräch dazu fortsetzen?“*\n",
    "\n",
    "**Beim nächsten Termin des Banking-Agenten:**\n",
    "\n",
    "*„Herr Müller, ich habe Ihre Situation berücksichtigt. Angesichts Ihrer Bedenken zur Inflation und des Zeithorizonts bis zu Lisas Studium habe ich eine ausgewogene Anlagestrategie vorbereitet. Möchten Sie, dass ich sie Ihnen vorstelle?“*\n",
    "\n",
    "Was auf den ersten Blick wie normaler Kundenservice wirkt, markiert in Wahrheit einen grundlegenden Paradigmenwechsel in der Entwicklung künstlicher Intelligenz. Das System reagiert nicht nur auf aktuelle Anfragen, sondern verfügt über ein funktionales Gedächtnis: Es erinnert sich an frühere Gespräche, erkennt Zusammenhänge zwischen Interaktionen und schafft so Kontinuität und Vertrauen in der Kundenbeziehung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c4409",
   "metadata": {},
   "source": [
    "### Warum ist Gedächtnis für agentenbasierte KI-Systeme entscheidend?\n",
    "\n",
    "Im Bank- und Versicherungswesen ist Vertrauen die Grundlage jeder Kundenbeziehung. Ein Kunde, der seine finanzielle\n",
    "Situation, Ziele und Sorgen immer wieder neu erklären muss, fühlt sich nicht verstanden.\n",
    "Ein Banking-Agent mit Gedächtnis hingegen:\n",
    "\n",
    "- **Erkennt Muster:** Wiederkehrende Gespräche, Sparziele, Relevanzprofile\n",
    "- **Lernt kontinuierlich:** Passt Empfehlungen an neue Lebenssituationen an\n",
    "- **Vergisst selektiv:** Behält Relevantes, verwirft Überflüssiges\n",
    "- **Priorisiert intelligent:** Fokussiert auf das Wesentliche in der Informationsflut\n",
    "\n",
    "### Die drei neurobiologischen Säulen, Selektive Gedächtnisfilterung, Plastische Gewichtsanpassung und Adaptives Vergessen\n",
    "\n",
    "Dieses Notebook zeigt, wie zentrale Mechanismen des menschlichen Gehirns auf Banking-Agenten übertragen werden können:\n",
    "\n",
    "1. **Selektive Gedächtnisfilterung** (Thalamus, Amygdala, Hippocampus, Präfrontaler Kortex)\n",
    "   *Notebook:* `selective_memory_filtering/selective_memory_filtering.ipynb`\n",
    "    - Trennt relevante von irrelevanten Kundengespräche\n",
    "    - Priorisiert wichtige Gesprächsinhalte und Lebensereignisse\n",
    "    - Berücksichtigt Kontext und zeitliche Dringlichkeit\n",
    "    - Verhindert impulsive Speicherung durch Konsolidierungs-Mechanismus\n",
    "\n",
    "2. **Plastische Gewichtsanpassung** (Synaptische Plastizität, LTP/LTD)\n",
    "   *Notebook:* `plastic_memory/plastic_memory.ipynb`\n",
    "    - Lernt aus Kundenfeedback und Gesprächsmustern\n",
    "    - Verstärkt erfolgreiche Empfehlungen\n",
    "    - Schwächt ineffektive Strategien ab\n",
    "\n",
    "3. **Adaptives Vergessen** (Ebbinghaus-Kurve, Interferenz, Homeostatic Scaling)\n",
    "   *Notebook:* `adaptive_forgetting/adaptive_forgetting.ipynb`\n",
    "    - Löscht veraltete Informationen zeitabhängig\n",
    "    - Ersetzt überholte Muster durch neue\n",
    "    - Bewahrt Systemkapazität und Reaktionsfähigkeit\n",
    "\n",
    "Ein solcher Banking-Agent steht exemplarisch für den Übergang von reaktiver Interaktion zu kontinuierlichem,\n",
    "kontextbewusstem Lernen – ein Schritt hin zu wirklich adaptiven, erinnernden KI-Systemen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ede9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9d915",
   "metadata": {},
   "source": [
    "## Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Notebook-Styling laden\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = (\n",
    "    NOTEBOOK_DIR.parent\n",
    "    if (NOTEBOOK_DIR.parent / \"notebook_style.py\").exists()\n",
    "    else NOTEBOOK_DIR\n",
    ")\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"notebook_style\", PROJECT_ROOT / \"notebook_style.py\"\n",
    ")\n",
    "if spec is None or spec.loader is None:\n",
    "    raise ImportError(\"notebook_style.py nicht gefunden\")\n",
    "\n",
    "nb_style = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"notebook_style\"] = nb_style\n",
    "spec.loader.exec_module(nb_style)\n",
    "\n",
    "PLOT_COLORS = nb_style.setup_plot_style(\n",
    "    aliases={\n",
    "        'correct': 'primary',\n",
    "        'incorrect': 'quaternary',\n",
    "        'prediction': 'secondary',\n",
    "        'stored': 'accent',\n",
    "    },\n",
    "    cycle_keys=(\"primary\", \"secondary\", \"accent\"),\n",
    ")\n",
    "\n",
    "SEED = int(nb_style.SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbfda1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three_pillars",
   "metadata": {},
   "source": [
    "### Die 3 Säulen: Mathematische Modelle\n",
    "\n",
    "#### *Säule 1: Selektive Filterung*\n",
    "**Salienz-Score:** S(t) = w_importance·importance_norm + w_recency·exp(−age/τ) + w_relevance·relevance\n",
    "- **importance_norm** = min(importance/1.0, 1.0) — Robuste Normierung\n",
    "- **exp(−age/τ)** — Exponentieller Abfall (Recency-Effekt)\n",
    "- **relevance** ∈ [0,1] — Relevanz-Score\n",
    "\n",
    "**Gating-Regel:** Speichere wenn S(t) ≥ θ; bei vollem Speicher (K Gespräche) ersetze das mit geringster Salienz.\n",
    "\n",
    "#### *Säule 2: Plastische Gewichte*\n",
    "**Vorhersage:** ŷ = σ(w·x) — Sigmoid-Aktivierung\n",
    "\n",
    "**Delta-Regel:** Δw = η(y − ŷ)x − αw\n",
    "- **η(y − ŷ)x** — Fehler-getriebenes Lernen (LTP bei y − ŷ > 0, LTD bei y − ŷ < 0)\n",
    "- **−αw** — Homeostase (Gewichtsdecay)\n",
    "\n",
    "#### *Säule 3: Adaptives Vergessen*\n",
    "**EMA mit adaptiver Lernrate:** m_t = (1−λ_t)·m_{t-1} + λ_t·x_t\n",
    "\n",
    "**Adaptive Lernrate:** λ_t = clip(λ_min + k·|e|/σ, λ_min, λ_max)\n",
    "- **|e|** = |y − ŷ| — Fehler\n",
    "- **σ** = std(|e|) — Fehler-Standardabweichung\n",
    "- **Peaks von λ_t** an Regimewechseln (Überraschungen)\n",
    "\n",
    "**Hinweis:** τ ≈ 1/λ für kleine λ; exakt: τ = −1/ln(1−λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0038e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenario",
   "metadata": {},
   "source": [
    "### Praktisches Szenario: Banking-Agent mit Kundengesprächen\n",
    "\n",
    "**Das Szenario: Herr Müller und sein digitaler Finanzberater**\n",
    "\n",
    "Stellen Sie sich vor, Herr Müller führt über mehrere Wochen hinweg Gespräche mit seinem Banking-Agent:\n",
    "\n",
    "- **Gespräch 1:** Herr Müller erwähnt, dass er Sorgen bezüglich der Inflation hat und dass die Ausbildung seiner Tochter Lisa in fünf Jahren finanziert werden muss. Er möchte eher vorsichtig investieren.\n",
    "- **Gespräch 2:** Er fragt nach Sparoptionen für Lisas Ausbildung.\n",
    "- **Gespräch 3:** Er berichtet von einer Erbschaft und möchte diese anlegen.\n",
    "- **Gespräch 4:** Er ist besorgt über Marktvolatilität und möchte seine Strategie überprüfen.\n",
    "\n",
    "**Das Problem für den Agent:**\n",
    "1. **Welche Gesprächsinhalte sind wichtig genug zum Speichern?** (Säule 1: Filterung)\n",
    "   - Lisas Ausbildung ist wichtig → speichern\n",
    "   - Tagesaktuelle Marktkommentare sind weniger wichtig → verwerfen\n",
    "\n",
    "2. **Wie lernt der Agent aus Feedback?** (Säule 2: Plastizität)\n",
    "   - Wenn Herr Müller sagt \"Diese Empfehlung war genau richtig\" → Gewichte verstärken\n",
    "   - Wenn er sagt \"Das passt nicht zu meiner Situation\" → Gewichte abschwächen\n",
    "\n",
    "3. **Wie vergisst der Agent alte Informationen?** (Säule 3: Vergessen)\n",
    "   - Alte Marktkommentare verblassen schnell\n",
    "   - Wichtige Ziele (Lisas Ausbildung) bleiben länger präsent\n",
    "   - Bei großen Veränderungen (z.B. Erbschaft) passt sich die Lernrate an\n",
    "\n",
    "**Features pro Gespräch:**\n",
    "- **importance:** Wichtigkeit des Gesprächsinhalts (0.0-1.0)\n",
    "  - Finanzielle Ziele: 0.8-1.0\n",
    "  - Persönliche Situation: 0.6-0.8\n",
    "  - Marktkommentare: 0.2-0.4\n",
    "- **recency:** Wie lange ist das Gespräch her (0-30 Tage)\n",
    "- **relevance:** Relevanz für aktuelle Empfehlung (0.0-1.0)\n",
    "\n",
    "**Feedback:**\n",
    "- **y = 1:** Empfehlung war hilfreich und passend\n",
    "- **y = 0:** Empfehlung war nicht hilfreich oder passte nicht\n",
    "\n",
    "**Ziel:** Der Agent lernt, bessere Empfehlungen zu geben, indem er:\n",
    "- Wichtige Gesprächsinhalte speichert (Säule 1)\n",
    "- Aus Feedback lernt (Säule 2)\n",
    "- Sich schnell an Veränderungen anpasst (Säule 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec0001",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec2a90",
   "metadata": {},
   "source": [
    "### Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktionen für die 3 Säulen\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid-Aktivierungsfunktion.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "\n",
    "def compute_salience(importance, age, relevance, w, tau, p95):\n",
    "    \"\"\"Berechnet Salienz-Score.\"\"\"\n",
    "    importance_norm = np.clip(importance / p95, 0.0, 1.0)\n",
    "    recency = np.exp(-age / tau)\n",
    "    s = w['importance'] * importance_norm + w['recency'] * recency + w['relevance'] * relevance\n",
    "    return float(np.clip(s, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def gate_memory(conversations, theta, k, w, tau, p95):\n",
    "    \"\"\"Filtert Gespräche und speichert.\"\"\"\n",
    "    memory = []\n",
    "    stored_indices = []\n",
    "    salience_scores = []\n",
    "\n",
    "    for i, conv in enumerate(conversations):\n",
    "        s = compute_salience(conv['importance'], conv['age'], conv['relevance'], w, tau, p95)\n",
    "        salience_scores.append(s)\n",
    "\n",
    "        if s >= theta:\n",
    "            if len(memory) < k:\n",
    "                memory.append(conv)\n",
    "                stored_indices.append(i)\n",
    "            else:\n",
    "                min_idx = np.argmin([compute_salience(m['importance'], m['age'], m['relevance'], w, tau, p95)\n",
    "                                     for m in memory])\n",
    "                memory[min_idx] = conv\n",
    "                stored_indices[min_idx] = i\n",
    "\n",
    "    return memory, stored_indices, salience_scores\n",
    "\n",
    "\n",
    "def delta_update(w, x, y, y_pred, eta, alpha):\n",
    "    \"\"\"Delta-Update für Gewichte.\"\"\"\n",
    "    error = y - y_pred\n",
    "    dw = eta * error * x - alpha * w\n",
    "    return w + dw\n",
    "\n",
    "\n",
    "def plot_results(y_true, y_pred, memory, lambda_t_list, theta, true_rate=None, memory_saliences=None):\n",
    "    \"\"\"Visualisiert die Ergebnisse der 3 Säulen.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 7))\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(range(len(y_true)), y_true, s=18, alpha=0.35, color=PLOT_COLORS.get('incorrect', '#d62728'), label='Beobachtungen (y)')\n",
    "    ax.plot(y_pred, label='Vorhersage (ŷ)', alpha=0.85, linewidth=2, color=PLOT_COLORS.get('prediction', '#ff7f0e'))\n",
    "    if true_rate is not None:\n",
    "        ax.plot(true_rate, label='Wahre Rate (nur Lernzwecke)', alpha=0.8, linestyle='--', color='black', linewidth=2)\n",
    "    ax.set_xlabel('Gesprächs-Index')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title('Säule 2 & 3: Vorhersage vs. Wahrheit')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    if memory_saliences is not None and len(memory_saliences) > 0:\n",
    "        vals = list(memory_saliences)\n",
    "        order = np.argsort(vals)[::-1]\n",
    "        vals = [vals[i] for i in order]\n",
    "        ax.barh(range(len(vals)), vals, color=PLOT_COLORS.get('stored', '#9467bd'), alpha=0.8)\n",
    "        ax.set_xlabel('Salienz S(t)')\n",
    "        ax.set_xlim(0, 1.0)\n",
    "    else:\n",
    "        vals = [m.get('importance', 0.0) for m in memory]\n",
    "        ax.barh(range(len(vals)), vals, color=PLOT_COLORS.get('stored', '#9467bd'), alpha=0.8)\n",
    "        ax.set_xlabel('Wichtigkeit')\n",
    "    ax.set_yticks(range(len(vals)))\n",
    "    ax.set_yticklabels([f'#{i+1}' for i in range(len(vals))], fontsize=9)\n",
    "    ax.axvline(theta, color=PLOT_COLORS.get('prediction', '#ff7f0e'), linestyle='--', label=f'θ={theta:.2f}')\n",
    "    ax.set_title(f'Säule 1: Selektive Filterung (K={len(vals)})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(lambda_t_list, color=PLOT_COLORS.get('stored', '#9467bd'), alpha=0.9, linewidth=2)\n",
    "    ax.set_xlabel('Gesprächs-Index')\n",
    "    ax.set_ylabel('λ_t (adaptive Lernrate)')\n",
    "    ax.set_title('Säule 3: Adaptives Vergessen')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    errors = np.abs(np.array(y_true) - np.array(y_pred))\n",
    "    ax.plot(errors, color=PLOT_COLORS.get('prediction', '#ff7f0e'), alpha=0.9, linewidth=2)\n",
    "    ax.fill_between(range(len(errors)), errors, alpha=0.15, color=PLOT_COLORS.get('prediction', '#ff7f0e'))\n",
    "    ax.set_xlabel('Gesprächs-Index')\n",
    "    ax.set_ylabel('Absoluter Fehler')\n",
    "    ax.set_title('Vorhersagefehler')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_conversations(t=300, drift_strength=0.0, seed=SEED):\n",
    "    \"\"\"Generiert Kundengespräche\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    conversations = []\n",
    "\n",
    "    betas1 = np.array([1.2, 0.6, 1.0])\n",
    "    betas2 = np.array([0.4, 1.2, 1.4])\n",
    "\n",
    "    def logit_from_rate(p):\n",
    "        eps=1e-6\n",
    "        p=float(np.clip(p, eps, 1-eps))\n",
    "        return np.log(p/(1-p))\n",
    "\n",
    "    for step in range(t):\n",
    "        # Basisrate mit Drift\n",
    "        base_rate = 0.20 if step < t // 2 else 0.20 + 0.30 * drift_strength\n",
    "        b0 = logit_from_rate(base_rate)\n",
    "\n",
    "        # Features pro Gespräch\n",
    "        importance = rng.uniform(0.2, 1.0)\n",
    "        age = rng.uniform(0, 30)\n",
    "        relevance = rng.uniform(0.0, 1.0)\n",
    "\n",
    "        # Normierte Features für das Logit\n",
    "        importance_norm = np.clip(importance, 0.0, 1.0)\n",
    "        recency = 1.0 - (age / 30.0)\n",
    "        x = np.array([importance_norm, recency, relevance])\n",
    "\n",
    "        # Phasenabhängige Betas\n",
    "        betas = betas1 if step < t // 2 else (1 - drift_strength) * betas1 + drift_strength * betas2\n",
    "\n",
    "        # Erfolgsrate und Label\n",
    "        logit = b0 + float(np.dot(betas, x))\n",
    "        true_rate = 1.0/(1.0 + np.exp(-np.clip(logit, -12, 12)))\n",
    "        y = 1 if rng.random() < true_rate else 0\n",
    "\n",
    "        conversations.append({'importance': importance, 'age': age, 'relevance': relevance, 'y': y, 'true_rate': true_rate})\n",
    "\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_md",
   "metadata": {},
   "source": [
    "### Interaktives Beispiel: Kundengespräche mit Herr Müller\n",
    "\n",
    "- Oben links: Beobachtungen (Punkte), Vorhersage (blau), wahre Rate (gestrichelt; nur Lernzwecke).\n",
    "- Oben rechts: Aktueller Speicher (Top-K) nach Salienz S(t) mit Schwelle θ.\n",
    "- Unten links: Adaptive Lernrate λ_t (steigt bei Überraschung).\n",
    "- Unten rechts: Absoluter Vorhersagefehler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83851f24471d4ae5abf7f44601eafbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='θ (Schwelle):', max=1.0, step=0.05), IntSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(theta=0.5, k=10, tau=10.0, eta=0.1, alpha=0.01, k_surprise=0.05)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = simulate_conversations(t=300, drift_strength=0.3, seed=SEED)\n",
    "importances = [conv['importance'] for conv in conversations]\n",
    "p95 = np.percentile(importances, 95)\n",
    "\n",
    "\n",
    "def update_plot(theta=0.5, k=10, tau=10.0, eta=0.1, alpha=0.01, k_surprise=0.05):\n",
    "    \"\"\"Aktualisiert die Plots basierend auf den Parametern.\"\"\"\n",
    "\n",
    "    w = {'importance': 0.35, 'recency': 0.35, 'relevance': 0.30}\n",
    "\n",
    "    total = w['importance'] + w['recency'] + w['relevance']\n",
    "    if total > 0:\n",
    "        w = {k: v/total for k,v in w.items()}\n",
    "\n",
    "    # Filtere Gespräche\n",
    "    memory, stored_indices, salience_scores = gate_memory(conversations, theta, k, w, tau, p95)\n",
    "\n",
    "    # Salienz der gespeicherten Gespräche extrahieren\n",
    "    memory_saliences = [salience_scores[i] for i in stored_indices]\n",
    "\n",
    "    # Simuliere das Lernen (Säule 2 & 3)\n",
    "    w_learn = np.array([0.5, 0.5, 0.5])\n",
    "    predictions = []\n",
    "    lambda_t_list = []\n",
    "    errors = []\n",
    "\n",
    "    lambda_min, lambda_max = 0.01, 0.3\n",
    "\n",
    "    for conv in conversations:\n",
    "        x = np.array([conv['importance'], conv['age'] / 30, conv['relevance']])\n",
    "        y_pred = sigmoid(np.dot(w_learn, x))\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "        # Fehler\n",
    "        error = abs(conv['y'] - y_pred)\n",
    "        errors.append(error)\n",
    "\n",
    "        # Adaptive Lernrate (Säule 3)\n",
    "        if len(errors) > 1:\n",
    "            sigma = np.std(errors[-min(20, len(errors)):])\n",
    "            lambda_t = np.clip(lambda_min + k_surprise * error / (sigma + 1e-6), lambda_min, lambda_max)\n",
    "        else:\n",
    "            lambda_t = lambda_min\n",
    "        lambda_t_list.append(lambda_t)\n",
    "\n",
    "        # Delta-Update (Säule 2)\n",
    "        # Effektive Lernrate durch adaptive Vergessensdynamik\n",
    "        eta_eff = max(1e-6, eta * (lambda_t / 0.3))\n",
    "        w_learn = delta_update(w_learn, x, conv['y'], y_pred, eta_eff, alpha)\n",
    "\n",
    "    # Visualisiere\n",
    "    true_rate_array = np.array([conv['true_rate'] for conv in conversations])\n",
    "    plot_results(np.array([conv['y'] for conv in conversations]), predictions, memory, lambda_t_list, theta, true_rate=true_rate_array, memory_saliences=memory_saliences)\n",
    "\n",
    "    # Statistik\n",
    "    print(\"\\nStatistik:\")\n",
    "    accepted = sum(s >= theta for s in salience_scores)\n",
    "    print(f\"  - Gespeicherte Gespräche: {len(memory)}/{k}\")\n",
    "    print(f\"  - Akzeptanzrate (S≥θ): {accepted/len(conversations)*100:.1f}%\")\n",
    "    print(f\"  - Durchschnittlicher Fehler: {np.mean(errors):.3f}\")\n",
    "    print(f\"  - Durchschnittliche λ_t: {np.mean(lambda_t_list):.3f}\")\n",
    "    print(f\"  - Parameter: θ={theta:.2f}, K={k}, τ={tau:.1f}, η={eta:.3f}, α={alpha:.3f}, k_surprise={k_surprise:.2f}\")\n",
    "\n",
    "\n",
    "# Interaktive Slider\n",
    "interact(\n",
    "    update_plot,\n",
    "    theta=widgets.FloatSlider(0.5, min=0.0, max=1.0, step=0.05,\n",
    "                               description='θ (Schwelle):'),\n",
    "    k=widgets.IntSlider(10, min=5, max=20, step=1,\n",
    "                         description='K (Kapazität):'),\n",
    "    tau=widgets.FloatSlider(10.0, min=1.0, max=30.0, step=1.0,\n",
    "                             description='τ (Recency):'),\n",
    "    eta=widgets.FloatSlider(0.1, min=0.01, max=0.5, step=0.05,\n",
    "                             description='η (Lernrate):'),\n",
    "    alpha=widgets.FloatSlider(0.01, min=0.0, max=0.1, step=0.01,\n",
    "                               description='α (Decay):'),\n",
    "    k_surprise=widgets.FloatSlider(0.05, min=0.0, max=1.0, step=0.05,\n",
    "                           description='k_surprise (Adaptivität):')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dbd278",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation",
   "metadata": {},
   "source": [
    "### Interpretation der Ergebnisse\n",
    "\n",
    "**Was beobachten wir?**\n",
    "\n",
    "**Säule 1 (Selektive Filterung):**\n",
    "- Hoher θ: Wenige Gespräche werden gespeichert (nur die wichtigsten)\n",
    "- Niedriger θ: Viele Gespräche werden gespeichert\n",
    "- Großes K: Speicher füllt sich langsamer\n",
    "- Kleines K: Speicher füllt sich schnell, alte Gespräche werden ersetzt\n",
    "\n",
    "**Säule 2 (Plastische Gewichte):**\n",
    "- Hohe η: Agent lernt schnell, aber kann überkorrigieren\n",
    "- Niedrige η: Agent lernt langsam, aber stabil\n",
    "- Hohe α: Gewichte verfallen schnell (Homeostase)\n",
    "- Niedrige α: Gewichte bleiben länger erhalten\n",
    "\n",
    "**Säule 3 (Adaptives Vergessen):**\n",
    "- Hohe k: Adaptive Lernrate reagiert stark auf Fehler (schnelle Anpassung)\n",
    "- Niedrige k: Adaptive Lernrate reagiert schwach (stabiles Lernen)\n",
    "- Peaks von λ_t: Zeigen Regimewechsel (Überraschungen)\n",
    "\n",
    "**Praxisleitfaden:**\n",
    "- **Konservativ:** Hoher θ, kleine K, niedrige η, hohe α → Stabile, vorsichtige Empfehlungen\n",
    "- **Ausgewogen:** Mittlerer θ, mittleres K, mittlere η, mittlere α → Balance zwischen Stabilität und Lernfähigkeit\n",
    "- **Aggressiv:** Niedriger θ, große K, hohe η, niedrige α → Schnelles Lernen, aber risikobehaftet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4acb424",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neurobiology",
   "metadata": {},
   "source": [
    "### Neurobiologische Grundlagen\n",
    "\n",
    "#### Warum funktioniert dieses Modell biologisch?\n",
    "\n",
    "**Säule 1: Salienz-gesteuerte Speicherung <-> Hippocampale/Neuromodulatorische Filter**\n",
    "- Das Gehirn speichert bevorzugt auffällige (salient) Ereignisse\n",
    "- Der Hippocampus bindet Ereignisse episodisch (Ort, Zeit, Kontext)\n",
    "- Neuromodulatoren (Dopamin, Noradrenalin) verstärken die Speicherung wichtiger Ereignisse\n",
    "- Dieses Modell: S(t) ≥ θ entscheidet über Speicherung\n",
    "\n",
    "**Säule 2: Plastische Gewichte <-> Synaptische Plastizität (LTP/LTD)**\n",
    "- Long-Term Potentiation (LTP): Synapsen werden stärker bei wiederholter Aktivierung\n",
    "- Long-Term Depression (LTD): Synapsen werden schwächer bei Inaktivität\n",
    "- Delta-Regel: Δw = η(y − ŷ)x − αw implementiert genau diesen Mechanismus\n",
    "- Positiver Fehler (y − ŷ > 0) → LTP (Gewichte erhöhen sich)\n",
    "- Negativer Fehler (y − ŷ < 0) → LTD (Gewichte verringern sich)\n",
    "- −αw als Homeostase: Verhindert, dass Gewichte unbegrenzt wachsen\n",
    "\n",
    "**Säule 3: Adaptives Vergessen <-> Ebbinghaus-Kurve & Adaptive Lernrate**\n",
    "- Ebbinghaus-Vergessenskurve: Gedächtnisspuren verblassen exponentiell\n",
    "- EMA: m_t = (1−λ_t)·m_{t-1} + λ_t·x_t beschreibt diesen Verfall\n",
    "- Adaptive Lernrate: λ_t erhöht sich bei Überraschungen (großen Fehlern)\n",
    "- Biologisch: Neuromodulatoren (Dopamin, Noradrenalin) erhöhen die Lernrate bei wichtigen Ereignissen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd427c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7121f84",
   "metadata": {},
   "source": [
    "#### Zusammenfassung\n",
    "Das Banking-Agent-Modell integriert drei zentrale neurobiologische Prinzipien – hippocampale Gedächtnisbildung, synaptische Plastizität und neuromodulatorische Steuerung – in einer praxisorientierten Architektur für agentenbasierte KI-Systeme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
