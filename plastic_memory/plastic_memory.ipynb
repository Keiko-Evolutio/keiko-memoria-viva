{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dc0d98",
   "metadata": {},
   "source": [
    "# Plastizität: Die Kunst der Anpassung\n",
    "\n",
    "Plastizität beschreibt die Fähigkeit von Erinnerungen und neuronalen Verbindungen, sich zu verändern: Sie passen sich neuen Erfahrungen an, verstärken sich, verblassen oder verschmelzen. Diese Eigenschaft ermöglicht es biologischen Systemen, aus Erfahrungen zu lernen und sich flexibel an veränderte Umgebungen anzupassen.\n",
    "\n",
    "Die neurobiologischen Grundlagen der Plastizität wurden durch Donald Hebb in seinem berühmten Postulat beschrieben:\n",
    "\n",
    "*„When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place such that A's efficiency as one of the cells firing B is increased.\"* \n",
    "\n",
    "Hebb betonte dabei Kausalität und zeitliche Präzedenz: Zelle A muss zur Aktivierung von Zelle B beitragen – nicht nur gleichzeitig aktiv sein. Die populäre Vereinfachung *„Cells that fire together, wire together.\"* (geprägt von Carla Shatz) erfasst diese kausale Beziehung nicht vollständig. \n",
    "\n",
    "Hebbian Learning manifestiert sich in zwei komplementären Prozessen: **Langzeitpotenzierung (LTP)**, die häufig genutzte synaptische Verbindungen stärkt, und **Langzeitdepression (LTD)**, die ungenutzte Verbindungen schwächt – das Prinzip *„Use it or lose it\"*.\n",
    "\n",
    "Diese Mechanismen haben unmittelbaren Einfluss auf die Entwicklung künstlicher neuronaler Netze. Die wiederholte Präsentation von Trainingsdaten über mehrere Epochen entspricht der synaptischen Verstärkung durch wiederholte Aktivierung. Je öfter ein neuronales Netz bestimmte Muster sieht, desto stärker werden die entsprechenden Gewichte angepasst – eine direkte Analogie zur biologischen Plastizität.\n",
    "\n",
    "In Multi-Agent-Systemen lässt sich Plastizität durch adaptive Gedächtnisarchitekturen umsetzen, die sich kontinuierlich reorganisieren. Dabei umfasst sie sowohl **strukturelle Plastizität** (Bildung und Auflösung von Verbindungen) als auch **funktionale Plastizität** (Anpassung bestehender Gewichte)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10be4da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ef9e7",
   "metadata": {},
   "source": [
    "## Synaptische Plastizität – Neurobiologische Grundlagen und praktische Anwendung\n",
    "\n",
    "### Problemstellung und Motivation\n",
    "\n",
    "Finanz- und Versicherungsunternehmen agieren in einer Umgebung, in der sich Betrugsmuster, Kundenerwartungen und regulatorische Rahmenbedingungen kontinuierlich verändern. Klassische regelbasierte Frühwarnsysteme sind auf historische Wissensstände beschränkt und verlieren rasch an Wirksamkeit, sobald Angreifer neue Taktiken anwenden oder Kundinnen und Kunden ihr Verhalten anpassen. Zudem führen Fehlalarme zu erhöhter operativer Belastung und beeinträchtigen das Kundenerlebnis.\n",
    "\n",
    "Die Neurobiologie zeigt mit der **synaptischen Plastizität**, dass lernfähige Systeme ihre Verbindungsstärken dynamisch anpassen können. **Langzeitpotenzierung (LTP)** verstärkt erfolgreiche Signalwege, während **Langzeitdepression (LTD)** ineffektive Verbindungen abschwächt. Diese Mechanismen gewährleisten, dass das Gehirn neue Muster integriert, ohne zuvor Gelerntes vollständig zu überschreiben (catastrophic forgetting). Überträgt man diese Prinzipien auf analytische Plattformen, entsteht ein adaptives Gedächtnis, das mathematisch nachvollziehbar auf Erfahrungswissen reagiert.\n",
    "\n",
    "Im Kontext der Betrugserkennung ermöglicht plastisches Lernen, feingranulare Wechselwirkungen zwischen Transaktionsmerkmalen zu bewerten und laufend zu aktualisieren. Zugleich lassen sich regulatorische Anforderungen an **Erklärbarkeit** und **Nachvollziehbarkeit** (Explainable AI, XAI) erfüllen.\n",
    "\n",
    "**Neurobiologischer Referenzpunkt:**  \n",
    "Mechanismen der synaptischen Plastizität (LTP, LTD) als biologische Grundlage adaptiver Informationsverarbeitung.\n",
    "\n",
    "### Praktische Relevanz im Finanzwesen\n",
    "\n",
    "Betrugserkennung, Kreditrisikomanagement und Compliance-Überwachung profitieren von kontinuierlich lernenden Modellen, die flexibel auf neue Muster reagieren und regulatorisch prüfbar bleiben.\n",
    "\n",
    "Die biologischen Prinzipien lassen sich auf analytische Plattformen übertragen und in drei Leitlinien verdichten:\n",
    "\n",
    "- **Koinzidenzdetektion als Merkmalsinteraktion:** Plastische Systeme bewerten nicht isolierte Features, sondern deren gemeinsame Aktivierung. In der Praxis bedeutet dies, Gewichtsmatrizen über Feature-Kombinationen zu führen.  \n",
    "- **Kalziumdynamik als adaptive Lernrate:** Leaky-Integrator-Modelle für Kalzium implementieren schwellenbasierte Regulation: Niedrige Ca²⁺-Level begünstigen LTD (Phosphatasen), hohe Ca²⁺-Level begünstigen LTP (Kinasen). Dies sichert Stabilität und vermeidet Überanpassung.  \n",
    "- **Schwellensteuerung als Governance:** Adaptive Schwellenwerte werden an Zielmetriken (z. B. gewünschte False-Alarm-Rate) ausgerichtet und schaffen auditierbare Eingriffspunkte für Risk- und Compliance-Teams.\n",
    "\n",
    "Diese methodischen Leitplanken bilden die Grundlage für die anschließende Fallstudie im Finanzwesen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793aeb80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Notebook-Styling laden\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = (\n",
    "    NOTEBOOK_DIR.parent\n",
    "    if (NOTEBOOK_DIR.parent / \"notebook_style.py\").exists()\n",
    "    else NOTEBOOK_DIR\n",
    ")\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"notebook_style\", PROJECT_ROOT / \"notebook_style.py\"\n",
    ")\n",
    "if spec is None or spec.loader is None:\n",
    "    raise ImportError\n",
    "\n",
    "nb_style = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"notebook_style\"] = nb_style\n",
    "spec.loader.exec_module(nb_style)\n",
    "\n",
    "PLOT_COLORS = nb_style.setup_plot_style(\n",
    "    aliases={\n",
    "        'prediction': 'primary',\n",
    "        'actual': 'secondary',\n",
    "        'weight': 'accent',\n",
    "        'error': 'quaternary',\n",
    "    },\n",
    "    cycle_keys=(\"primary\", \"secondary\", \"accent\"),\n",
    ")\n",
    "\n",
    "SEED = int(getattr(nb_style, 'SEED', 42))\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe5ebb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ltp_ltd",
   "metadata": {},
   "source": [
    "### LTP/LTD\n",
    "\n",
    "#### *Langzeitpotenzierung (LTP) – Verstärkung*\n",
    "Wenn ein Feature **koaktiv** mit einem **positiven Fehler** ist, wird die Verbindung stärker.\n",
    "\n",
    "#### *Langzeitdepression (LTD) – Abschwächung*\n",
    "Wenn ein Feature **koaktiv** mit einem **negativen Fehler** ist, wird die Verbindung schwächer.\n",
    "\n",
    "#### *Die Delta-Regel (Error-modulierte Hebb'sche Regel)*\n",
    "\n",
    "**Vorhersage:**\n",
    "$$\\hat{y} = \\sigma(w \\cdot x)$$\n",
    "\n",
    "**Gewichtsupdate:**\n",
    "$$\\Delta w = \\eta (y - \\hat{y}) x - \\alpha w$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\hat{y}$ = Vorhersage (geschätztes Risiko)\n",
    "- $w$ = Gewichte (Synapsenstärken)\n",
    "- $x$ = Features (Eingaben)\n",
    "- $\\eta$ = Lernrate (wie schnell wir lernen)\n",
    "- $(y - \\hat{y})$ = Fehler (Fehler-Signal)\n",
    "- $\\alpha$ = Decay-Rate (Homeostase, verhindert Überanpassung)\n",
    "- $\\sigma$ = Sigmoid-Aktivierungsfunktion\n",
    "\n",
    "**Neurobiologisches Mapping:**\n",
    "- **LTP:** Wenn $y > \\hat{y}$ (positiver Fehler) -> Dopamin-Signal -> Kinasen aktiv -> Gewicht steigt\n",
    "- **LTD:** Wenn $y < \\hat{y}$ (negativer Fehler) -> Phosphatasen aktiv -> Gewicht sinkt\n",
    "- **Homeostase:** $-\\alpha w$ verhindert ungebremstes Wachstum (Homeostatic Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d7f28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenario",
   "metadata": {},
   "source": [
    "### Praktisches Szenario: Betrugserkennung\n",
    "\n",
    "**Problem:** Eine Bank möchte Betrug erkennen. Aber Betrüger ändern ständig ihre Taktiken.\n",
    "\n",
    "**Lösung:** Ein adaptives System, das von Fehlern lernt.\n",
    "\n",
    "**Features (4 Beispiele):**\n",
    "- $x_1$ = \"neues Gerät\" (0 oder 1)\n",
    "- $x_2$ = \"Nacht\" (0 oder 1)\n",
    "- $x_3$ = \"Ausland\" (0 oder 1)\n",
    "- $x_4$ = \"hoher Betrag\" (0 oder 1)\n",
    "\n",
    "**Label:**\n",
    "- $y$ = 1 (Betrug) oder 0 (legitim)\n",
    "\n",
    "**Streaming mit Drift (Regimewechsel):**\n",
    "- Phase 1 (t=0-100): Betrugsbasisrate = 5%\n",
    "- Phase 2 (t=100-200): Betrugsbasisrate = 15% (Betrüger ändern Taktik!)\n",
    "- Phase 3 (t=200-300): Betrugsbasisrate = 8%\n",
    "\n",
    "Das System muss sich an diese Veränderungen anpassen – genau wie das Gehirn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive_weights",
   "metadata": {},
   "source": [
    "### Adaptive Gewichte\n",
    "\n",
    "Statt fester Regeln verwenden wir **adaptive Gewichte**, die sich an neue Informationen anpassen:\n",
    "\n",
    "$$\\Delta w = \\eta (y - \\hat{y}) x - \\alpha w$$\n",
    "\n",
    "**Was passiert?**\n",
    "\n",
    "1. **Vorhersage:** Wir schätzen das Risiko: $\\hat{y} = \\sigma(w \\cdot x)$\n",
    "\n",
    "2. **Fehler berechnen:** Wie falsch war unsere Vorhersage? $error = y - \\hat{y}$\n",
    "\n",
    "3. **LTP (Verstärkung):** Wenn $error > 0$ (wir unterschätzt haben):\n",
    "   - Koaktivierte Features werden verstärkt\n",
    "   - Gewichte steigen\n",
    "   - Nächstes Mal werden wir vorsichtiger\n",
    "\n",
    "4. **LTD (Abschwächung):** Wenn $error < 0$ (wir überschätzt haben):\n",
    "   - Koaktivierte Features werden abgeschwächt\n",
    "   - Gewichte sinken\n",
    "   - Nächstes Mal werden wir weniger vorsichtig\n",
    "\n",
    "5. **Homeostase:** Der Term $-\\alpha w$ verhindert, dass Gewichte unbegrenzt wachsen\n",
    "   - Erhält die Lernfähigkeit des Systems\n",
    "   - Biologisch: Homeostatic Scaling\n",
    "\n",
    "**Neurobiologisches Mapping:**\n",
    "- Fehler-Signal $(y - \\hat{y})$ <-> Dopamin-Signal\n",
    "- Koaktivität $(x, error)$ <-> LTP/LTD\n",
    "- Homeostase $-\\alpha w$ <-> Rezeptor-Normalisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2c591",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b339eec",
   "metadata": {},
   "source": [
    "### Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid-Aktivierungsfunktion.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "\n",
    "def simulate_stream(t=300, drift_strength=0.0, seed=42):\n",
    "    \"\"\"Generiert Streaming-Daten.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # 4 binäre Features\n",
    "    x = rng.integers(0, 2, size=(t, 4)).astype(float)\n",
    "\n",
    "    # Basisrate je Phase\n",
    "    base = np.zeros(t)\n",
    "    base[:100] = 0.05\n",
    "    base[100:200] = 0.05 + drift_strength*0.10\n",
    "    base[200:] = 0.08\n",
    "\n",
    "    # Feature-Wirkungen je Phase\n",
    "    betas1 = np.array([0.8,  0.2,  0.1,  0.4])\n",
    "    betas2 = np.array([0.1,  0.9,  0.5,  0.2])\n",
    "    betas3 = np.array([0.3,  0.3,  0.6,  0.3])\n",
    "\n",
    "    def logit_from_rate(p):\n",
    "        eps=1e-6\n",
    "        p=np.clip(p, eps, 1-eps)\n",
    "        return np.log(p/(1-p))\n",
    "\n",
    "    logit = np.zeros(t)\n",
    "    for step in range(t):\n",
    "        if step < 100:\n",
    "            betas = betas1\n",
    "            b0 = logit_from_rate(base[step])\n",
    "        elif step < 200:\n",
    "            betas = betas2\n",
    "            b0 = logit_from_rate(base[step])\n",
    "        else:\n",
    "            betas = betas3\n",
    "            b0 = logit_from_rate(base[step])\n",
    "        logit[step] = b0 + np.dot(betas, x[step])\n",
    "\n",
    "    true_rate = 1.0/(1.0 + np.exp(-np.clip(logit, -20, 20)))\n",
    "    y = rng.random(t) < true_rate\n",
    "\n",
    "    return x.astype(float), y.astype(float), true_rate.astype(float)\n",
    "\n",
    "\n",
    "def step_update(w, x_t, y_t, y_pred_t, eta=0.1, alpha=0.01):\n",
    "    \"\"\"Aktualisiert Gewichte nach Delta-Regel.\"\"\"\n",
    "    error = y_t - y_pred_t\n",
    "    delta_w = eta * error * x_t - alpha * w\n",
    "    return w + delta_w\n",
    "\n",
    "\n",
    "def plot_results(y, y_preds, weights_history, true_rate=None):\n",
    "    \"\"\"Visualisiert Vorhersagen und Gewichte.\"\"\"\n",
    "    T = len(y)\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(20, 7),\n",
    "        sharex=True,\n",
    "        gridspec_kw={'height_ratios': [3, 2]}\n",
    "    )\n",
    "\n",
    "    if true_rate is not None:\n",
    "        ax1.plot(true_rate, 'k--', lw=2, alpha=0.8, label='Wahre Rate (nur Lernzwecke)')\n",
    "\n",
    "    ax1.scatter(np.arange(T), y, color=PLOT_COLORS.get('neutral', '#7f7f7f'), s=10, alpha=0.35, label='Beobachtungen (y)')\n",
    "    ax1.plot(y_preds, color=PLOT_COLORS['prediction'], label='Vorhersage (ŷ)', lw=2, alpha=0.8)\n",
    "\n",
    "    if true_rate is not None:\n",
    "        ax1.fill_between(np.arange(T), 0, true_rate, alpha=0.08, color=PLOT_COLORS.get('actual', '#ff7f0e'))\n",
    "    ax1.set_ylabel('Betrugsbasisrate', fontsize=11, fontweight='bold')\n",
    "    ax1.legend(loc='upper left', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_title('Betrugserkennung: Adaptive Gewichte mit Delta-Regel', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    feature_names = ['neues Gerät', 'Nacht', 'Ausland', 'hoher Betrag']\n",
    "    colors = [PLOT_COLORS['weight'], PLOT_COLORS['accent'], PLOT_COLORS['primary'], PLOT_COLORS['secondary']]\n",
    "    for i in range(4):\n",
    "        ax2.plot(weights_history[:, i], label=f'w_{i+1} ({feature_names[i]})', lw=2, color=colors[i])\n",
    "\n",
    "    ax2.set_ylabel('Gewichte w_i', fontsize=11, fontweight='bold')\n",
    "    ax2.set_xlabel('Zeit (t)', fontsize=11, fontweight='bold')\n",
    "    ax2.legend(loc='upper left', ncol=2, fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(0, color='black', linestyle='--', alpha=0.3, lw=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1463c",
   "metadata": {},
   "source": [
    "### Interaktives Beispiel: Gewichte-Anpassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demo_code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d5a743a12a49beac2fafe9008e475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='η (Lernrate):', max=0.5, min=0.01, step=0.01), Float…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(eta=0.1, alpha=0.01, drift_strength=0.0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data, y_data, true_rate_data = simulate_stream(t=300, drift_strength=0.0)\n",
    "\n",
    "\n",
    "def update_plot(eta=0.1, alpha=0.01, drift_strength=0.0):\n",
    "    \"\"\"Aktualisiert die Plots basierend auf den Parametern.\"\"\"\n",
    "    x, y, true_rate = simulate_stream(t=300, drift_strength=drift_strength, seed=SEED)\n",
    "\n",
    "    w = np.zeros(4)\n",
    "    y_preds = np.zeros(300)\n",
    "    weights_history = np.zeros((300, 4))\n",
    "\n",
    "    for t in range(300):\n",
    "        y_pred = sigmoid(np.dot(w, x[t]))\n",
    "        y_preds[t] = y_pred\n",
    "\n",
    "        w = step_update(w, x[t], y[t], y_pred, eta=eta, alpha=alpha)\n",
    "        weights_history[t] = w.copy()\n",
    "\n",
    "    plot_results(y, y_preds, weights_history, true_rate=true_rate)\n",
    "\n",
    "# Interaktive Slider\n",
    "interact(\n",
    "    update_plot,\n",
    "    eta=widgets.FloatSlider(0.1, min=0.01, max=0.50, step=0.01,\n",
    "                             description='η (Lernrate):'),\n",
    "    alpha=widgets.FloatSlider(0.01, min=0.001, max=0.05, step=0.001,\n",
    "                               description='α (Decay):'),\n",
    "    drift_strength=widgets.FloatSlider(0.0, min=0.0, max=1.0, step=0.1,\n",
    "                                        description='Drift-Stärke:')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f339d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation",
   "metadata": {},
   "source": [
    "### Interpretation der Ergebnisse\n",
    "\n",
    "**Was beobachten wir?**\n",
    "\n",
    "1. **Hohe Lernrate (η = 0.3-0.5):**\n",
    "   - Schnelle Anpassung an Regimewechsel\n",
    "   - Aber: Mehr Rauschen, Gewichte schwingen stärker\n",
    "   - Resultat: Überanpassung möglich\n",
    "\n",
    "2. **Niedrige Lernrate (η = 0.01-0.05):**\n",
    "   - Stabile Vorhersage\n",
    "   - Aber: Langsame Anpassung an Regimewechsel\n",
    "   - Resultat: Verzögerter Lerneffekt\n",
    "\n",
    "3. **Hohe Homeostase (α = 0.03-0.05):**\n",
    "   - Gewichte bleiben klein und stabil\n",
    "   - Aber: Weniger Plastizität, schwächere Anpassung\n",
    "   - Resultat: Konservatives Lernen\n",
    "\n",
    "4. **Niedrige Homeostase (α = 0.001-0.005):**\n",
    "   - Mehr Plastizität, stärkere Anpassung\n",
    "   - Aber: Gewichte können unbegrenzt wachsen\n",
    "   - Resultat: Aggressive Anpassung\n",
    "\n",
    "5. **Mit Drift (Regimewechsel):**\n",
    "   - Phase 1 (5% Betrug): System lernt die Basisrate\n",
    "   - Phase 2 (15% Betrug): System passt sich an (wenn η hoch genug)\n",
    "   - Phase 3 (8% Betrug): System passt sich wieder an\n",
    "\n",
    "**Praxisleitfaden:**\n",
    "- **Konservativ:** Hohe α, niedrige η -> Stabil, aber langsam\n",
    "- **Aggressiv:** Niedrige α, hohe η -> Schnell, aber rauschig\n",
    "- **Ausgewogen:** Mittlere Werte -> Balance zwischen Stabilität und Responsivität"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2304b56",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neurobiologie",
   "metadata": {},
   "source": [
    "### Neurobiologische Grundlagen\n",
    "\n",
    "#### Warum funktioniert dieses Modell biologisch?\n",
    "\n",
    "**1. Delta-Regel <-> Fehler-getriebenes Lernen**\n",
    "- Das Gehirn nutzt Fehler-Signale (Dopamin) um zu lernen\n",
    "- Große Fehler -> Starke Anpassung\n",
    "- Kleine Fehler -> Schwache Anpassung\n",
    "- Dies ist biologisch plausibel und experimentell bestätigt\n",
    "\n",
    "**2. LTP (Langzeitpotenzierung) <-> Verstärkung**\n",
    "- Wenn präsynaptische Aktivität (x) mit postsynaptischem Fehler-Signal (y - ŷ) koaktiv ist\n",
    "- Und der Fehler positiv ist (wir unterschätzt haben)\n",
    "- Dann wird die Synapse stärker (LTP)\n",
    "- Biologisch: Dopamin-Signal aktiviert Kinasen -> Rezeptoren werden stärker\n",
    "\n",
    "**3. LTD (Langzeitdepression) <-> Abschwächung**\n",
    "- Wenn präsynaptische Aktivität mit postsynaptischem Fehler-Signal koaktiv ist\n",
    "- Und der Fehler negativ ist (wir überschätzt haben)\n",
    "- Dann wird die Synapse schwächer (LTD)\n",
    "- Biologisch: Fehler-Signal aktiviert Phosphatasen -> Rezeptoren werden schwächer\n",
    "\n",
    "**4. Homeostase (−αw) <-> Homeostatic Scaling**\n",
    "- Der Term −αw verhindert ungebremstes Wachstum\n",
    "- Biologisch: Rezeptor-Normalisierung, Synapsen-Sättigung vermeiden\n",
    "- Erhält die Lernfähigkeit des Systems\n",
    "\n",
    "**5. Koaktivität (x, error) <-> Hebb'sche Regel**\n",
    "- Nur wenn präsynaptische Aktivität UND postsynaptisches Fehler-Signal gleichzeitig aktiv sind\n",
    "- Wird die Synapse modifiziert\n",
    "- Dies ist die biologische Basis der Hebb'schen Regel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba401f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98394a",
   "metadata": {},
   "source": [
    "#### Zusammenfassung\n",
    "Die Delta-Regel mit Homeostase integriert zentrale neurobiologische Prinzipien – Langzeitpotenzierung und -depression (LTP/LTD), fehlergetriebenes Lernen und homeostatisches Scaling – zu einem kohärenten, anwendungsorientierten Lernmodell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
